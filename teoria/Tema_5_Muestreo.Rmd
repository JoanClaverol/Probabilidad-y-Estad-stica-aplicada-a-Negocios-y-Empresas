---
title: "Tema 5 - Muestreo estadístico y estimación puntual"
author: "Ricardo Alberich, Juan Gabriel Gomila y Arnau Mir"
date: ''
output: 
  ioslides_presentation:
    widescreen: true
    css: Mery_style.css
    logo: Images/matriz_mov.gif
---

\newcommand{\FunCar}{\phi}
\newcommand{\Momk}{M}
\newcommand{\MomCenk}{MC}
\newcommand\momento{m}
\newcommand{\momentocentral}{\mu}


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Tipos de muestreo

## Muestreo aleatorio con y sin reposición {-}

<l class="definition">Muestreo aleatorio</l>: consiste en seleccionar una muestra de la población de manera que todas las muestras del mismo tamaño sean **equiprobables**.


Consideremos una urna de 100 bolas numeradas del 1 al 100:

<div class="center">
```{r, echo=FALSE, label=urna,fig.cap="Una urna de 100 bolas",out.width = "800px"}
knitr::include_graphics("Images/basev.png",dpi=400)
```
</div>

## Muestreo aleatorio con y sin reposición {-}

Queremos extraer una muestra de 15 bolas. Para ello, podríamos repetir 15 veces el proceso de sacar una bola de la urna, anotar su número y devolverla a la urna. El tipo de muestra obtenida de esta manera recibe el nombre de **muestra aleatoria con reposición**, o simple (una **m.a.s.**, para abreviar).
  
<div class="center">

```{r, echo=FALSE, label=simple,fig.cap="Una muestra aleatoria simple",out.width = "700px"}
knitr::include_graphics("Images/simplev.png",dpi=400)
```
</div>


## Muestreo aleatorio sin reposición {-}
<l class="definition">Muestra aleatoria sin reposición</l>: Otra manera de extraer nuestra muestra sería repetir 15 veces el proceso de sacar una bola de la urna pero ahora sin devolverla. En este caso se habla de una **muestra aleatoria sin reposición**.

<div class="center">
```{r, echo=FALSE, label=sinrep,fig.cap="Una muestra aleatoria sin reposición",out.width = "800px"}
knitr::include_graphics("Images/sinrepv.png",dpi=400)
```
</div>

## Muestras aleatorias con reposición vs. sin reposición

<l class="observ"> Observación</l>: ¿Cuándo se puede considerar equivalente válido realizar una muestra con reposición que sin reposición?

Si el tamaño de la población es muy grande en relación al de la muestra (por dar una regla, digamos que, al menos, unas 1000 veces mayor).




## Muestreo sistemático

<l class="definition">Muestreo sistemático</l>: Supongamos que los individuos de una población vienen dados en forma de una lista ordenada. El **muestreo sistemático** consiste en tomarlos a intervalos constantes escogiendo al azar el primer individuo que elegimos.

<div class="center">
```{r, echo=FALSE, label=sist,fig.cap="Una muestra aleatoria sistemática",out.width = "800px"}
knitr::include_graphics("Images/sistv.png",dpi=400)
```
</div>

## Muestreo sistemático

La figura anterior describe una muestra aleatoria sistemática de 15 bolas de nuestra urna de 100 bolas: hemos empezado a escoger por la bola roja oscura, que ha sido elegida al azar, y a partir de ella hemos tomado 1 de cada 7 bolas, volviendo al principio cuando hemos llegado al final de la lista de bolas.


## Muestreo aleatorio estratificado {-}

<l class="definition">Muestreo aleatorio estratificado</l>: Este tipo de muestreo se utiliza cuando la población está clasificada en  **estratos** que son de interés para la propiedad estudiada. Se toma una muestra aleatoria de cada estrato y se unen en una muestra global. A este proceso se le llama **muestreo aleatorio estratificado**.

Supongamos que nuestra urna de 100 bolas contiene 40 bolas de un color y 60 de otro color tal como muestra la figura:

<div class="center">
```{r, echo=FALSE, label=estratprevi,fig.cap="Una muestra aleatoria estratificada con dos estratos",out.width = "800px"}
knitr::include_graphics("Images/estrat.png",dpi=400)
```
</div>

## Muestreo aleatorio estratificado {-}

Para tomar una muestra aleatoria estratificada de 15 bolas, considerando como estratos los dos colores, tomaríamos una muestra aleatoria de 6 bolas del primer color y una muestra aleatoria de 9 bolas del segundo color. 


## Muestreo por conglomerados {-}

El proceso de obtener y estudiar una muestra aleatoria en algunos casos es caro o difícil, incluso aunque dispongamos de la lista completa de la población. 

<l class="definition"> Muestreo por conglomerados</l>: una alternativa posible sería, en vez de extraer una muestra aleatoria de todos los individuos de la población, escoger primero al azar unos subconjuntos en los que la población está dividida, a las que llamamos en este contexto **conglomerados** (*clusters*).

## Muestreo por conglomerados {-}
Supongamos que las 100 bolas de nuestra urna se agrupan en 20 conglomerados de 5 bolas cada uno según las franjas verticales.

Para obtener una muestra aleatoria por conglomerados de tamaño 15, escogeríamos al azar 3 conglomerados y la muestra estaría formada por sus bolas: los conglomerados escogidos están marcados en azul:

<div class="center">
```{r, echo=FALSE, label=clust,fig.cap="Una muestra aleatoria por conglomerados con 2 estratos y 20 conglomerados",out.width = "600px"}
knitr::include_graphics("Images/cluster.png",dpi=400)
```
</div>



## Muestreo polietápico

<l class="definition"> Muestreo polietápico</l>: si una vez seleccionada la muestra aleatoria de conglomerados, tomamos de alguna manera una muestra aleatoria de cada uno de ellos, estaremos realizando un **muestreo polietápico**.

La figura muestra un ejemplo sencillo de muestreo polietápico de nuestra urna: hemos elegido al azar 5 conglomerados (marcados en azul) y de cada uno de ellos hemos elegido 3 bolas al azar sin reposición.

<div class="center">
```{r, echo=FALSE, label=poli,fig.cap="Una muestra polietápica de 5 conglomerados y 3 bolas al azar sin reposición",out.width = "600px"}
knitr::include_graphics("Images/poli.png",dpi=400)
```
</div>

# Estadísticos

## Definición formal de muestra aleatoria simple

<l class="definition"> Una m.a.s. de tamaño $n$ (de una v.a. $X$)</l> es

* un conjunto de $n$ copias independientes de $X$, o

* un conjunto de $n$ variables aleatorias independientes $X_1,\ldots,X_n$, todas con la distribución de $X$.

<l class="definition"> Una realización de una m.a.s.</l> son los $n$ valores $x_1,\ldots,x_n$ que  toman las v.a. $X_1,\ldots,X_n$.


## Definición formal de estadístico
<l class="definition"> Un  *estadístico* $T$</l> es una función aplicada a la muestra $X_1,\ldots,X_n$:
$$
T=f(X_1,\ldots,X_n)
$$
Este estadístico se aplica a las realizaciones  de la muestra

Así pues, un **estadístico** es una (otra) variable aleatoria, con distribución, esperanza, etc.

La **distribución muestral** de $T$ es la distribución de  esta variable aleatoria.

Estudiando  esta distribución muestral, podremos estimar propiedades de $X$ a partir del comportamiento de una muestra.


<l class="definition">Error estándar de $T$</l>: desviación típica de $T$.

# Media muestral

## Definición de media muestral

<l class="definition">Media muestral </l>: sea $X_1,\ldots, X_n$ una m.a.s.\ de tamaño $n$ de una v.a.\ $X$ de esperanza $\mu_X$ y desviación típica $\sigma_X$

La *media muestral* es:
$$
\overline{X}=\frac{X_1+\cdots+X_n}{n}
$$

<div class="prop">
Proposición
</div>
En estas condiciones,
$$
E(\overline{X})=\mu_X,\quad \sigma_{\overline{X}}=\frac{\sigma_X}{\sqrt{n}}
$$
donde $\sigma_{\overline{X}}$ es el **error estándar** de $\overline{X}$.

## Propiedades de la media muestral

<div class="prop">
Proposición
</div>

* Es un estimador puntual de $\mu_X$

* $E(\overline{X})=\mu_X$: el valor esperado de $\overline{X}$ es $\mu_X$.

* Si tomamos muchas veces una m.a.s. y calculamos la media muestral, el valor medio  de estas medias tiende con mucha probabilidad a ser $\mu_X$.

* $\sigma_{\overline{X}}= \sigma_X/\sqrt{n}$: la variabilidad de los resultados de $\overline{X}$ tiende a 0  a medida que tomamos muestras más grandes.


# Teorema Central del Límite

## Teorema Central del Límite
<l class="prop">Teorema Central del Límite. </l>
Sea $X_1,\ldots, X_n$ una m.a.s. de una v.a. $X$ **cualquiera** de esperanza $\mu_X$ y desviación típica $\sigma_X$. Cuando $n\to \infty$, 
$$
\overline{X}\to N\Big(\mu_X,\frac{\sigma_X}{\sqrt{n}}\Big)
$$
y por lo tanto
$$
Z=\frac{\overline{X}-\mu_X}{\frac{\sigma_X}{\sqrt{n}}}\to N(0,1)
$$

<l class="observ">Caso $n$ grande</l>:
Si $n$ es grande (**$n\geq 30$ o 40**), 
$\overline{X}$ es aproximadamente normal, con esperanza  $\mu_X$ y desviación típica  $\dfrac{\sigma_X}{\sqrt{n}}$


## Teorema Central del Límite

<div class="example-sol">

```{r echo=FALSE, fig.align='center'}
set.seed(1001)
valores.medios.long.pétalo=replicate(10000,mean(sample(iris$Petal.Length,40,
                                                      replace =TRUE)))
hist(valores.medios.long.pétalo,freq=FALSE, main="Histograma 
 de las medias de 10000 muestras\n de tamaño 40 de una población",xlab="Valores medios de las muestras",ylab="Densidad",ylim=c(0,1.5))
lines(density(valores.medios.long.pétalo),lty=2,lwd=2,col="red")
x=sort(iris$Petal.Length)
lines(x,dnorm(x,mean(iris$Petal.Length),sd(iris$Petal.Length)/sqrt(40)),lty=3,lwd=2,col="blue")
legend("topright",legend=c("densidad","normal"),
 lwd=c(2,2),lty=c(2,3),col=c("red","blue"))
```
</div>

## Media muestral en muestras sin reposición

Sea $X_1,\ldots, X_n$ una m.a. **sin  reposición** de tamaño $n$ de una v.a. $X$ de esperanza $\mu_X$ y desviación típica $\sigma_X$. 

Si $n$ es  pequeño en relación al tamaño $N$ de la población, todo lo que hemos contado funciona (aproximadamente).

Si $n$ es grande en relación a $N$, entonces
$$
E(\overline{X})=\mu_X,\quad \sigma_{\overline{X}}=\frac{\sigma_X}{\sqrt{n}}\cdot \sqrt{\frac{N-n}{N-1}}
$$
(**factor de población finita**)

El Teorema Central del Límite ya no funciona exactamente en este último caso.

# Proporción muestral
## Proporción muestral. Definición

<l class="definition">Proporción muestral. </l>
Sea $X$ una v.a. Bernoulli de parámetro  $p_X$ (1 éxito, 0 fracaso). Sea $X_1,\ldots,X_n$ una m.a.s. de tamaño $n$ de $X$. 

$S=\sum_{i=1}^n X_i$ es el nombre de éxitos observados
es $B(n,p)$.

La **proporción muestral** es 
$$
\widehat{p}_X=\frac{S}{n}
$$
y es un estimador de $p_X$.

Notemos que $\widehat{p}_X$ es un caso particular de $\overline{X}$, por lo que todo lo que hemos dicho para medias muestrales es cierto para proporciones muestrales.

## Proporción muestral. Propiedades

<l class="prop">Proposición</l>

* Valor esperado de la proporción muestral: 
$$E(\widehat{p}_X)=p_X$$

* **Error estándar** de la proporción muestral: 

$$\displaystyle \sigma_{\widehat{p}_X}=\sqrt{\frac{p_X(1-p_X)}{n}}$$

* Si la muestra es sin reposición y $n$ es relativamente grande,

$$\displaystyle\sigma_{\widehat{p}_X}=\sqrt{\frac{p_X(1-p_X)}{n}}\cdot{\sqrt{\frac{N-n}{N-1}}}.$$

## Proporción muestral. Propiedades

<l class="prop"> Teorema:</l>
Si $n$ es grande ($n\geq 30$ o 40) y la muestra es aleatoria simple, usando el Teorema Central del Límite,
$$
\frac{\widehat{p}_X-p_X}{\sqrt{\frac{{p}_X(1-{p}_X)}{n}}}\approx N(0,1)
$$



# Varianza muestral y desviación típica muestral

## Varianza muestral y desviación típica muestral. Definición
<l class="definition"> Varianza muestral y desviación típica muestral.</l> Sea $X_1,\ldots, X_n$ una m.a.s. de tamaño $n$ de una v.a. $X$ de esperanza $\mu_X$ y desviación típica $\sigma_X$.

La **varianza muestral** es
$$
\widetilde{S}_{X}^2=\frac{\sum\limits_{i=1}^n (X_{i}-\overline{X})^2}{n-1}
$$
La **desviación típica muestral** es 
$$
\widetilde{S}_{X}=+\sqrt{\widetilde{S}_{X}^2}
$$

## Varianza muestral y desviación típica muestral. Definición

Además, escribiremos
$$
S^2_{X}=\frac{\sum\limits_{i=1}^n (X_{i}-\overline{X})^2}{n}=\frac{(n-1)}{n}\widetilde{S}^2_{X}\quad\mbox{ y }\quad S_X=+\sqrt{S_X^2}
$$

## Varianza muestral y desviación típica muestral. Propiedades
<l class="prop">Propiedades</l>
$$\displaystyle S^2_X=\frac{\sum\limits_{i=1}^n (X_{i}-\overline{X})^2}{n}=\left(\frac{\sum\limits_{i=1}^n
X_{i}^2}{n}-\overline{X}^2\right)$$


$$\displaystyle \widetilde{S}_{X}^2=\frac{n}{n-1}\left(\frac{\sum\limits_{i=1}^n
X_{i}^2}{n}-\overline{X}^2\right)$$


## Varianza muestral y desviación típica muestral. Propiedades
<l class="prop"> Teorema. </l>
Si la v.a. $X$ es normal, entonces $E(\widetilde{S}_{X}^2)=\sigma_{X}^2$ y 
la v.a.
$$
\frac{(n-1)\widetilde{S}_{X}^2}{\sigma_{X}^2}
$$
tiene distribución $\chi_{n-1}^2$.

## La distribución $\chi_{n-1}^2$

<l class="prop">Distribución $\chi_n^2$ </l>

La distribución $\chi_n^2$ ($\chi$: en catalán, **khi**; en castellano, **ji**; en inglés, **chi**), donde $n$  es un parámetro llamado  **grados de libertad**:

es la de 
$$
X=Z_{1}^{2}+Z_{2}^{2}+\cdots +Z_{n}^{2}
$$ 
donde  $Z_{1},Z_{2},\ldots, Z_{n}$ son v.a. independientes  $N(0,1)$.

## La distribución $\chi_{n-1}^2$. Propiedades

<l class="prop">Propiedades $\chi_n^2$ </l>

* Su función de densidad es:
$$
f_{\chi_n^2}(x)={\frac{1}{2^{n/2} \Gamma (n/2)}} x^{(n/2)-1} e^{-x/2},\quad\mbox{ si $x\geq 0$}
$$
donde $\Gamma(x)=\int_{0}^{\infty} t^{x-1}e^{-t}\, dt$, si $x> 0$.

* Si $X_{\chi_n^2}$ es una v.a.\ con distribución  $\chi_n^2$,
$$E\left(X_{\chi_n^2}\right)=n,\quad Var\left(X_{\chi_n^2}\right)=2 n$$

* ${\chi_n^2}$ se aproxima a una distribución normal $N\left(n,\sqrt{2n}\right)$ para  $n$ grande
($n>40$ o $50$).

## La distribución $\chi_{n-1}^2$. Gráficos

El gráfico de la función de densidad de distintas distribuciones $\chi^2_n$ para $n=1,2,3,4,5,10$ se puede observar en el gráfico siguiente:
<div class="center">
```{r echo=FALSE}
x=seq(from=0,to=20,by=0.1)
plot(x,dchisq(x,3),type="l",ylab="función de densidad",xlab="",col="red")
lines(x,dchisq(x,2),type="l",col="blue")
lines(x,dchisq(x,1),type="l",col="black")
lines(x,dchisq(x,4),type="l",col="yellow")
lines(x,dchisq(x,5),type="l",col="green")
lines(x,dchisq(x,10),type="l",col="orange")

legend("topright",legend=c("n=1","n=2","n=3","n=4","n=5","n=10"),
 lty=rep(1,6),col=c("black","blue","red","yellow","green","orange"))
```
</center>

# Propiedades de los estimadores
## Estimadores insesgados

¿Cuándo un estimador es bueno?

<l class="definition">Estimadores insesgados</l>
Un estimador puntual $\widehat{\theta}$ de un parámetro  poblacional
$\theta$ es  **insesgado, no sesgado o sin sesgo** cuando su valor esperado es precisamente el valor del parámetro:
$$
E(\widehat{\theta})=\theta
$$ 
Entonces se dice que el estimador puntual es **no sesgado**.


El  **sesgo** de $\widehat{\theta}$ es la diferencia 
$$E(\widehat{\theta})-\theta$$


## Estimadores insesgados. Ejemplos

<l class="prop">Proposición</l>


* $\overline{X}$ es estimador no sesgado de $\mu_X$: $E(\overline{X})=\mu_X$.


* $\widehat{p}_X$ es estimador no sesgado de $p_X$: $E(\widehat{p}_X)=p_X$.


* Si $X$ es normal: $\widetilde{S}_{X}^2$ es estimador no sesgado de $\sigma_X^2$: $E(\widetilde{S}_{X}^2)=\sigma_X^2$ 


* Si $X$ es normal: $E({S}_{X}^2)=\dfrac{n-1}{n}\sigma_X^2$. Por lo tanto ${S}_{X}^2$, es sesgado, con sesgo
$$
E({S}_{X}^2)-\sigma_X^2=\dfrac{n-1}{n}\sigma_X^2-\sigma_X^2=-\dfrac{\sigma_X^2}{n}\mbox{ que tiende a  }0.
$$


## Estimadores eficientes

¿Cuando un estimador es **bueno**?

Cuando es no segado y tiene poca variabilidad  (así es más probable que aplicado a una m.a.s. dé un valor más cercano al valor esperado)


<l class="definition"> Error estándar de un estimador</l> $\widehat{\theta}$:  es su desviación típica
$$\sigma_{\widehat{\theta}}=\sqrt{Var(\widehat{\theta})}$$

## Estimadores eficientes

<l class="definition">Eficiencia de un estimador</l>
Dados dos estimadores $\widehat{\theta}_1$, $\widehat{\theta}_2$ no sesgados (o con sesgo  que tiende a $0$) del mismo parámetro  $\theta$, diremos que $\widehat{\theta}_1$ es **más eficiente** que $\widehat{\theta}_2$ cuando

$$\sigma_{\widehat{\theta}_1}< \sigma_{\widehat{\theta}_2},$$  es decir, cuando $$Var(\widehat{\theta}_1)< Var(\widehat{\theta}_2).$$


## Estimadores máximo verosímiles

¿Cómo encontramos buenos estimadores?

Antes de explicar la metodología, necesitamos una definición previa:

<l class="definition">Función de verosimilitud de la muestra.</l>
Sea $X$ una v.a. **discreta** con función de probabilidad $f_X(x;\theta)$
que depende de un parámetro  desconocido $\theta$.

Sea $X_{1},\ldots X_{n}$ una m.a.s. de $X$, y sea $x_1,x_2,\ldots,x_n$ una realización de esta muestra.

La **función de verosimilitud** de la muestra es la probabilidad condicionada siguiente:
$$
\begin{array}{ll}
{L(\theta|x_1,x_2,\ldots,x_n)} & := P(x_1,x_2,\ldots,x_n|\theta)=P(X_1=x_1)\cdots P(X_n=x_n) \\ & = f_X(x_1;\theta)\cdots f_X(x_n;\theta).
\end{array}
$$


## Estimadores máximo verosímiles
Dada la función de verosimilitud $L(\theta|x_1,\ldots,x_n)$ de la muestra, indicaremos por  $\hat{\theta}(x_1,\ldots,x_n)$ el valor del parámetro  $\theta$ en el que  se alcanza  el máximo
de $L(\theta|x_1,\ldots,x_n)$. Será una función de $x_1,\ldots,x_n$.

<l class="definition">Estimador máximo verosímil.</l>
Un estimador $\hat{\theta}$ de un parámetro  $\theta$ es **máximo verosímil** (**MV**) cuando, para cada m.a.s, la probabilidad de observarlo es máxima, cuando el parámetro toma el valor del estimador aplicado a la muestra,  es decir, si la función de verosimilitud
$$L(\theta|x_1,x_2,\ldots,x_n)= P(x_1,x_2,\ldots,x_n|\theta)$$ alcanza su máximo.

